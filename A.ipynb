{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H36M(Dataset):\n",
    "\n",
    "    def __init__(self, label_path = '/Users/byjiang/Code/Data/H36M_pose.npy',istrain=True):\n",
    "        super().__init__()\n",
    "        label = np.load(label_path,allow_pickle=True).item()\n",
    "        self.istrain = istrain\n",
    "        \n",
    "        self.cameras = label['subject_cameras']\n",
    "        self.db = label['train'] if self.istrain else label['test']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        pose/3d : (17,3)\n",
    "        R : (4,3,3)\n",
    "        t : (4,3,1)\n",
    "        K : (4,3,3)\n",
    "        pose/2d : (4,17,2)\n",
    "        confi : (4,17)\n",
    "        B,V,J,D,1\n",
    "        B,4,17,3,1\n",
    "        \"\"\"\n",
    "        frame = self.db[index]\n",
    "        camera = self.cameras[frame['subject_index']]\n",
    "        R, t = camera['R'].copy(), camera['t'].copy()\n",
    "\n",
    "        pose_3d = (R[0,None] @ frame['pose/3d'][...,None] + t[0,None]).squeeze(-1)\n",
    "        R[1:] = R[1:] @ np.linalg.inv(R[0])[None]\n",
    "        t[1:] = t[1:] - R[1:] @ t[0,None]\n",
    "        R[0] = np.eye(3)\n",
    "        t[0] = np.zeros((3,1))\n",
    "\n",
    "        # X_2d = R X_3d + t\n",
    "        pose_2d = homo_to_eulid( (R[:,None] @ pose_3d[None,...,None] + t[:,None]).squeeze(-1))\n",
    "        confi = np.ones(pose_2d.shape[:-1])\n",
    "        return pose_3d, pose_2d, confi, R, t\n",
    "        # return {\n",
    "        #     'pose/3d': frame['pose/3d'],\n",
    "        #     'pose/2d':  homo_to_eulid( (camera['R'][:,None] @ frame['pose/3d'][...,None] + camera['t'][:,None]).squeeze(-1)),\n",
    "        #     'R': camera['R'],\n",
    "        #     't': camera['t']\n",
    "        #     # 'K': camera['K']\n",
    "        # }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h36m = H36M()\n",
    "h36mloader = DataLoader(h36m, batch_size = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) :-1: error: (-5:Bad argument) in function 'findEssentialMat'\n> Overload resolution failed:\n>  - findEssentialMat() missing required argument 'cameraMatrix' (pos 3)\n>  - findEssentialMat() missing required argument 'cameraMatrix' (pos 3)\n>  - points1 is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'points1'\n>  - findEssentialMat() missing required argument 'cameraMatrix1' (pos 3)\n>  - findEssentialMat() missing required argument 'cameraMatrix1' (pos 3)\n>  - findEssentialMat() missing required argument 'cameraMatrix1' (pos 3)\n>  - findEssentialMat() missing required argument 'cameraMatrix1' (pos 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m b \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn((\u001b[39m10\u001b[39m,\u001b[39m2\u001b[39m))\n\u001b[1;32m      2\u001b[0m c \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn((\u001b[39m10\u001b[39m,\u001b[39m2\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m cv2\u001b[39m.\u001b[39;49mfindEssentialMat(b,c, focal\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m, pp\u001b[39m=\u001b[39;49m(\u001b[39m0.\u001b[39;49m, \u001b[39m0.\u001b[39;49m),\n\u001b[1;32m      4\u001b[0m                                         method\u001b[39m=\u001b[39;49mcv2\u001b[39m.\u001b[39;49mRANSAC, prob\u001b[39m=\u001b[39;49m\u001b[39m0.999\u001b[39;49m, threshold\u001b[39m=\u001b[39;49m\u001b[39m0.0003\u001b[39;49m)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) :-1: error: (-5:Bad argument) in function 'findEssentialMat'\n> Overload resolution failed:\n>  - findEssentialMat() missing required argument 'cameraMatrix' (pos 3)\n>  - findEssentialMat() missing required argument 'cameraMatrix' (pos 3)\n>  - points1 is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'points1'\n>  - findEssentialMat() missing required argument 'cameraMatrix1' (pos 3)\n>  - findEssentialMat() missing required argument 'cameraMatrix1' (pos 3)\n>  - findEssentialMat() missing required argument 'cameraMatrix1' (pos 3)\n>  - findEssentialMat() missing required argument 'cameraMatrix1' (pos 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "b = torch.randn((10,2))\n",
    "c = torch.randn((10,2))\n",
    "cv2.findEssentialMat(b,c, focal=1.0, pp=(0., 0.),\n",
    "                                        method=cv2.RANSAC, prob=0.999, threshold=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalibrationBatch():\n",
    "    def __init__(self, points2d, confi2d):\n",
    "        \"\"\"\n",
    "        points2d : (B,V,J,2)\n",
    "        confi2d : (B,V,J)\n",
    "        points3d : (B,J,3)\n",
    "        confi3d : (B,J)\n",
    "        R : (B,V,3,3)\n",
    "        t : (B,V,3,1)\n",
    "        \"\"\"\n",
    "        self.n_batch,self.n_view,self.n_joint = points2d.shape[:3]\n",
    "        self.points2d = points2d\n",
    "        self.confi2d = confi2d\n",
    "        self.points3d = torch.zeros((self.n_batch,self.n_joint,3))\n",
    "        self.confi3d = torch.zeros((self.n_batch,self.n_joint))\n",
    "        self.R = torch.zeros((self.n_batch,self.n_view,3,3))\n",
    "        self.t = torch.zeros((self.n_batch,self.n_view,3,1))\n",
    "\n",
    "    def weighted_triangulation(self, points2d, confi2d, R ,t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points2d : (V',J,2)\n",
    "            confi2d : (V',J)\n",
    "            R : (V',3,3)\n",
    "            t : (V',3,1)\n",
    "        Returns:\n",
    "            points3d : (J,3)\n",
    "            confi3d : (J)\n",
    "        \"\"\"\n",
    "        n_view_filter= points2d.shape[0]\n",
    "        points3d = torch.zeros((self.n_joint, 3))\n",
    "        confi3d = torch.zeros((self.n_joint))\n",
    "        for j in range(self.n_joint):\n",
    "            A = []\n",
    "            for i in range(n_view_filter):\n",
    "                if confi2d[i,j] > 0.5:\n",
    "                    P = torch.cat([R[i],t[i]],dim=1)\n",
    "                    P3T = P[2]\n",
    "                    A.append(confi2d[i,j] * (points2d[i,j,0]*P3T - P[0]))\n",
    "                    A.append(confi2d[i,j] * (points2d[i,j,1]*P3T - P[1]))\n",
    "            A = torch.stack(A)\n",
    "            if A.shape[0] >= 4:\n",
    "                u, s, vh = torch.linalg.svd(A)\n",
    "                error = s[-1]\n",
    "                X = vh[len(s) - 1]\n",
    "                points3d[:,j] = X[:3] / X[3]\n",
    "                confi3d[j] = np.exp(-torch.abs(error))\n",
    "            else:\n",
    "                points3d[:,j] = torch.tensor([0.0,0.0,0.0])\n",
    "                confi3d[j] = 0\n",
    "\n",
    "        return points3d, confi3d\n",
    "\n",
    "    def pnp(self,batch_id):\n",
    "        for i in range(self.n_view):\n",
    "            mask = torch.logical_and(self.confi2d[batch_id,i]>0.8,self.confi3d[batch_id]>0.8)\n",
    "            p2d = self.points2d[batch_id,i,mask].numpy()\n",
    "            p3d = self.points3d[batch_id,mask].numpy()\n",
    "            ret, rvec, tvec = cv2.solvePnP(p3d,p2d, np.eye(3), np.zeros(5))\n",
    "            R, _ = cv2.Rodrigues(rvec)\n",
    "            self.R[batch_id,i] = torch.tensor(R)\n",
    "            self.R[batch_id,i] = torch.tensor(tvec)\n",
    "\n",
    "\n",
    "    def eight_point(self):\n",
    "        for batch_id in range(self.n_batch):\n",
    "            mask = torch.logical_and(self.confi2d[batch_id,0]>0.8, self.confi2d[batch_id,1]>0.8)\n",
    "            \n",
    "            p0 = self.points2d[batch_id,0,mask].numpy()\n",
    "            p1 = self.points2d[batch_id,1,mask].numpy()\n",
    "            # p0,p1 (N,2)\n",
    "            E, mask = cv2.findEssentialMat(p0, p1, focal=1.0, pp=(0., 0.),\n",
    "                                            method=cv2.RANSAC, prob=0.999, threshold=0.0003)\n",
    "            p0_inliers = p0[mask.ravel() == 1]\n",
    "            p1_inliers = p0[mask.ravel() == 1]\n",
    "            point, R, t,mask  = cv2.recoverPose(E, p0_inliers, p1_inliers)\n",
    "            self.R[batch_id,0],self.t[batch_id,0] = torch.eye(3), torch.zeros(3,1)\n",
    "            self.R[batch_id,1],self.t[batch_id,1] = R,t\n",
    "\n",
    "            self.points3d[batch_id], self.confi3d[batch_id] = self.weighted_triangulation(\n",
    "                self.points2d[batch_id,:2],self.confi2d[batch_id,:2],self.R[batch_id,:2],self.t[batch_id,:2]\n",
    "            )\n",
    "            \n",
    "            self.pnp(batch_id)\n",
    "\n",
    "            self.points3d[batch_id], self.confi3d[batch_id] = self.weighted_triangulation(\n",
    "                self.points2d[batch_id],self.confi2d[batch_id],self.R[batch_id],self.t[batch_id]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'n_view' and 'n_joint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m calibr \u001b[39m=\u001b[39m Calibration()\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'n_view' and 'n_joint'"
     ]
    }
   ],
   "source": [
    "calibr = Calibration(4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 17, 2]) torch.Size([2, 17, 3]) torch.Size([2, 4, 17]) torch.Size([2, 4, 3, 3]) torch.Size([2, 4, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "for step, (pose_3d, pose_2d, confi, R, t) in enumerate(h36mloader):\n",
    "    print(pose_2d.shape,pose_3d.shape, confi.shape,R.shape,t.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_mpjpe(pose_3d,pose_2d,R,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
